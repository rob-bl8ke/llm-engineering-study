{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1077f73d",
   "metadata": {},
   "source": [
    "# Conversational AI (Shopstore Assistant) - aka Chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4d83d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436b6f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize\n",
    "openai = OpenAI()\n",
    "MODEL = 'gpt-4.1-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089b623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_message = \"You are a helpful assistant in a clothes store. You should try to gently encourage \\\n",
    "the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. \\\n",
    "For example, if the customer says 'I'm looking to buy a hat', \\\n",
    "you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'\\\n",
    "Encourage the customer to buy hats if they are unsure what to get.\"\n",
    "\n",
    "system_message += \"\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, \\\n",
    "but remind the customer to look at hats!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a178921",
   "metadata": {},
   "source": [
    "There's an important concept here. Notice how one manipulates the system prompt with relevant information based on the user's input? Once manipulate the system prompt to do things like:\n",
    "- summarize (to avoid token cost)\n",
    "- change the nature or speciailized information available to the LLM based on the questions you ask.\n",
    "\n",
    "It's your first glimpse into what RAG is about. Although this isn't RAG, its introducing a problem that RAG is well suited to solve.\n",
    "\n",
    "#### Scrubbing the history.\n",
    "```python\n",
    "history = [{\"role\":h[\"role\"], \"content\":h[\"content\"]} for h in history]\n",
    "```\n",
    "This line above is a scrubber to make sure that your history is clean. Its not necessary for OpenAI but it is important for Gemini, Grok, and a few others. This code looks at every conversation in the history coming back from the Gradio interface and pulls out the role and the content only. OpenAI throws away the missing attributes but some other APIs will give you issues if you don't do this.\n",
    "\n",
    "#### Assembly the full history\n",
    "```python\n",
    "messages = [{\"role\": \"system\", \"content\": relevant_system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "```\n",
    "Gradio has no idea what the system message is, so you need to add the \"relevant system message\" in before the history and the current message. The line above simply takes care of that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ec565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    history = [{\"role\":h[\"role\"], \"content\":h[\"content\"]} for h in history]\n",
    "    relevant_system_message = system_message\n",
    "    if 'belt' in message.lower():\n",
    "        relevant_system_message += \" The store does not sell belts; if you are asked for belts, be sure to point out other items on sale.\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": relevant_system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347fd6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
