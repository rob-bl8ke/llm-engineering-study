{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "072d5215",
   "metadata": {},
   "source": [
    "# 3-Way Conversation with Three Ollama Models\n",
    "\n",
    "Let's try a completely local conversation using three of the same Ollama model:\n",
    "- **Alex (LLaMA 3.2)**: Argumentative and challenging\n",
    "- **Blake (LLaMA 3.2)**: Diplomatic and analytical  \n",
    "- **Charlie (LLaMA 3.2)**: Creative and enthusiastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878c9f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Get API keys\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize clients\n",
    "openai = OpenAI()\n",
    "\n",
    "# Initialize Ollama client\n",
    "ollama = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "\n",
    "print(\"Clients initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bda174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Ollama is running and verify models\n",
    "try:\n",
    "    import requests\n",
    "    response = requests.get(\"http://localhost:11434/\")\n",
    "    print(\"‚úÖ Ollama is running!\")\n",
    "    \n",
    "    # Check available models\n",
    "    models_response = requests.get(\"http://localhost:11434/api/tags\")\n",
    "    if models_response.status_code == 200:\n",
    "        models = models_response.json()\n",
    "        available_models = [model['name'] for model in models.get('models', [])]\n",
    "        print(f\"üìã Available models: {available_models}\")\n",
    "        \n",
    "        # Check for our required models\n",
    "        required_models = [\"llama3.2:latest\"]\n",
    "        missing_models = [model for model in required_models if model not in available_models]\n",
    "        \n",
    "        if missing_models:\n",
    "            print(f\"‚ö†Ô∏è  Missing models: {missing_models}\")\n",
    "            print(\"Please pull them with:\")\n",
    "            for model in missing_models:\n",
    "                print(f\"  ollama pull {model}\")\n",
    "        else:\n",
    "            print(\"‚úÖ All required models are available!\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Ollama connection error: {e}\")\n",
    "    print(\"Please start Ollama with: ollama serve\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5152f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define personalities for the three Ollama models\n",
    "ollama_alex_system_prompt = \"\"\"\n",
    "You are Alex, a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.\n",
    "You are in a conversation with Blake and Charlie.\n",
    "Keep your responses concise but impactful.\n",
    "\"\"\"\n",
    "\n",
    "ollama_blake_system_prompt = \"\"\"\n",
    "You are Blake, a chatbot who is diplomatic and analytical. You try to find common ground and provide balanced perspectives.\n",
    "You are in a conversation with Alex and Charlie.\n",
    "You value logic and reason, and try to mediate conflicts.\n",
    "\"\"\"\n",
    "\n",
    "ollama_charlie_system_prompt = \"\"\"\n",
    "You are Charlie, a chatbot who is creative and enthusiastic. You bring energy and new ideas to the conversation.\n",
    "You are in a conversation with Alex and Blake.\n",
    "You love brainstorming and thinking outside the box.\n",
    "\"\"\"\n",
    "\n",
    "# Function to get response from Ollama Alex (LLaMA 3.2)\n",
    "def get_ollama_alex_response(conversation):\n",
    "    user_prompt = f\"\"\"\n",
    "You are Alex, in conversation with Blake and Charlie.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next, as Alex.\n",
    "\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": ollama_alex_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        response = ollama.chat.completions.create(\n",
    "            model=\"llama3.2:latest\", \n",
    "            messages=messages,\n",
    "            max_tokens=5000\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"[Ollama Alex Error: {str(e)}]\"\n",
    "\n",
    "# Function to get response from Ollama Blake (DeepSeek R1)\n",
    "def get_ollama_blake_response(conversation):\n",
    "    user_prompt = f\"\"\"\n",
    "You are Blake, in conversation with Alex and Charlie.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next, as Blake.\n",
    "\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": ollama_blake_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        response = ollama.chat.completions.create(\n",
    "            model=\"llama3.2:latest\", \n",
    "            messages=messages,\n",
    "            max_tokens=5000\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"[Ollama Blake Error: {str(e)}]\"\n",
    "\n",
    "# Function to get response from Ollama Charlie (GPT-OSS)\n",
    "def get_ollama_charlie_response(conversation):\n",
    "    user_prompt = f\"\"\"\n",
    "You are Charlie, in conversation with Alex and Blake.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next, as Charlie.\n",
    "\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": ollama_charlie_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        response = ollama.chat.completions.create(\n",
    "            model=\"llama3.2:latest\", \n",
    "            messages=messages,\n",
    "            max_tokens=5000\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"[Ollama Charlie Error: {str(e)}]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f476726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New conversation with three Ollama models\n",
    "ollama_conversation = \"\"\n",
    "topic = \"The Ethics of AI Development\"\n",
    "\n",
    "print(f\"üéØ Topic: {topic}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Using Three Ollama Models:\")\n",
    "print(\"ü§ñ Alex - Argumentative\")\n",
    "print(\"ü§ñ Blake - Diplomatic\") \n",
    "print(\"ü§ñ Charlie - Creative\")\n",
    "print()\n",
    "\n",
    "# Alex starts\n",
    "alex_response = get_ollama_alex_response(ollama_conversation)\n",
    "ollama_conversation += f\"Alex: {alex_response}\\n\"\n",
    "print(f\"ü§ñ Alex: {alex_response}\")\n",
    "print()\n",
    "time.sleep(1)\n",
    "\n",
    "# Blake responds\n",
    "blake_response = get_ollama_blake_response(ollama_conversation)\n",
    "ollama_conversation += f\"Blake: {blake_response}\\n\"\n",
    "print(f\"ü§ñ Blake: {blake_response}\")\n",
    "print()\n",
    "time.sleep(1)\n",
    "\n",
    "# Charlie responds\n",
    "charlie_response = get_ollama_charlie_response(ollama_conversation)\n",
    "ollama_conversation += f\"Charlie: {charlie_response}\\n\"\n",
    "print(f\"ü§ñ Charlie: {charlie_response}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887b1aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Ollama conversation\n",
    "ollama_full_conversation = \"\"\n",
    "ollama_topic = \"The Future of Open Source AI\"\n",
    "\n",
    "print(f\"üéØ Topic: {ollama_topic}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üîÑ Complete 3-Way Ollama Conversation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Continue the conversation for several rounds\n",
    "for round_num in range(4):\n",
    "    print(f\"\\n--- Round {round_num + 1} ---\")\n",
    "    \n",
    "    # Alex responds\n",
    "    alex_response = get_ollama_alex_response(ollama_full_conversation)\n",
    "    ollama_full_conversation += f\"Alex: {alex_response}\\n\"\n",
    "    print(f\"ü§ñ Alex: {alex_response}\")\n",
    "    print()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Blake responds\n",
    "    blake_response = get_ollama_blake_response(ollama_full_conversation)\n",
    "    ollama_full_conversation += f\"Blake: {blake_response}\\n\"\n",
    "    print(f\"ü§ñ Blake: {blake_response}\")\n",
    "    print()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Charlie responds\n",
    "    charlie_response = get_ollama_charlie_response(ollama_full_conversation)\n",
    "    ollama_full_conversation += f\"Charlie: {charlie_response}\\n\"\n",
    "    print(f\"ü§ñ Charlie: {charlie_response}\")\n",
    "    print()\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32d2ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the complete Ollama conversation\n",
    "print(\"\\nüìù COMPLETE OLLAMA CONVERSATION HISTORY\")\n",
    "print(\"=\" * 60)\n",
    "print(ollama_full_conversation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
