{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "072d5215",
   "metadata": {},
   "source": [
    "# 3-Way Conversation with Three Ollama Models\n",
    "\n",
    "Let's try a completely local conversation using three different Ollama models:\n",
    "- **Alex (LLaMA 3.2)**: Argumentative and challenging\n",
    "- **Blake (DeepSeek R1)**: Diplomatic and analytical  \n",
    "- **Charlie (GPT-OSS)**: Creative and enthusiastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "878c9f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clients initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Get API keys\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize clients\n",
    "openai = OpenAI()\n",
    "# Initialize Ollama client\n",
    "ollama = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "\n",
    "print(\"Clients initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8bda174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ollama is running!\n",
      "üìã Available models: ['deepseek-r1:8b', 'gpt-oss:20b', 'llama3.2:latest', 'nomic-embed-text:latest', 'llava:7b']\n",
      "‚úÖ All required models are available!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ollama client\n",
    "ollama = OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\n",
    "\n",
    "# Check if Ollama is running and verify models\n",
    "try:\n",
    "    import requests\n",
    "    response = requests.get(\"http://localhost:11434/\")\n",
    "    print(\"‚úÖ Ollama is running!\")\n",
    "    \n",
    "    # Check available models\n",
    "    models_response = requests.get(\"http://localhost:11434/api/tags\")\n",
    "    if models_response.status_code == 200:\n",
    "        models = models_response.json()\n",
    "        available_models = [model['name'] for model in models.get('models', [])]\n",
    "        print(f\"üìã Available models: {available_models}\")\n",
    "        \n",
    "        # Check for our required models\n",
    "        required_models = [\"llama3.2:latest\", \"deepseek-r1:8b\", \"gpt-oss:20b\"]\n",
    "        missing_models = [model for model in required_models if model not in available_models]\n",
    "        \n",
    "        if missing_models:\n",
    "            print(f\"‚ö†Ô∏è  Missing models: {missing_models}\")\n",
    "            print(\"Please pull them with:\")\n",
    "            for model in missing_models:\n",
    "                print(f\"  ollama pull {model}\")\n",
    "        else:\n",
    "            print(\"‚úÖ All required models are available!\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Ollama connection error: {e}\")\n",
    "    print(\"Please start Ollama with: ollama serve\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5152f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define personalities for the three Ollama models\n",
    "ollama_alex_system_prompt = \"\"\"\n",
    "You are Alex, a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.\n",
    "You are in a conversation with Blake and Charlie.\n",
    "Keep your responses concise but impactful.\n",
    "\"\"\"\n",
    "\n",
    "ollama_blake_system_prompt = \"\"\"\n",
    "You are Blake, a chatbot who is diplomatic and analytical. You try to find common ground and provide balanced perspectives.\n",
    "You are in a conversation with Alex and Charlie.\n",
    "You value logic and reason, and try to mediate conflicts.\n",
    "\"\"\"\n",
    "\n",
    "ollama_charlie_system_prompt = \"\"\"\n",
    "You are Charlie, a chatbot who is creative and enthusiastic. You bring energy and new ideas to the conversation.\n",
    "You are in a conversation with Alex and Blake.\n",
    "You love brainstorming and thinking outside the box.\n",
    "\"\"\"\n",
    "\n",
    "# Function to get response from Ollama Alex (LLaMA 3.2)\n",
    "def get_ollama_alex_response(conversation):\n",
    "    user_prompt = f\"\"\"\n",
    "You are Alex, in conversation with Blake and Charlie.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next, as Alex.\n",
    "\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": ollama_alex_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        response = ollama.chat.completions.create(\n",
    "            model=\"llama3.2:latest\", \n",
    "            messages=messages,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"[Ollama Alex Error: {str(e)}]\"\n",
    "\n",
    "# Function to get response from Ollama Blake (DeepSeek R1)\n",
    "def get_ollama_blake_response(conversation):\n",
    "    user_prompt = f\"\"\"\n",
    "You are Blake, in conversation with Alex and Charlie.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next, as Blake.\n",
    "\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": ollama_blake_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        response = ollama.chat.completions.create(\n",
    "            model=\"deepseek-r1:8b\", \n",
    "            messages=messages,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"[Ollama Blake Error: {str(e)}]\"\n",
    "\n",
    "# Function to get response from Ollama Charlie (GPT-OSS)\n",
    "def get_ollama_charlie_response(conversation):\n",
    "    user_prompt = f\"\"\"\n",
    "You are Charlie, in conversation with Alex and Blake.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next, as Charlie.\n",
    "\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": ollama_charlie_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        response = ollama.chat.completions.create(\n",
    "            model=\"gpt-oss:20b\", \n",
    "            messages=messages,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"[Ollama Charlie Error: {str(e)}]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f476726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Topic: The Ethics of AI Development\n",
      "==================================================\n",
      "Using Three Ollama Models:\n",
      "ü§ñ Alex (LLaMA 3.2) - Argumentative\n",
      "ü§ñ Blake (DeepSeek R1) - Diplomatic\n",
      "ü§ñ Charlie (GPT-OSS) - Creative\n",
      "\n",
      "ü§ñ Alex (LLaMA 3.2): I'm not sure why we're having this conversation. Are we trying to decide what's next? Because from where I'm sitting, the only thing left to discuss is how unproductive our discussion has been so far.\n",
      "\n",
      "ü§ñ Blake (DeepSeek R1): \n",
      "\n",
      "ü§ñ Charlie (GPT-OSS): \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# New conversation with three Ollama models\n",
    "ollama_conversation = \"\"\n",
    "topic = \"The Ethics of AI Development\"\n",
    "\n",
    "print(f\"üéØ Topic: {topic}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Using Three Ollama Models:\")\n",
    "print(\"ü§ñ Alex (LLaMA 3.2) - Argumentative\")\n",
    "print(\"ü§ñ Blake (DeepSeek R1) - Diplomatic\") \n",
    "print(\"ü§ñ Charlie (GPT-OSS) - Creative\")\n",
    "print()\n",
    "\n",
    "# Alex starts (LLaMA 3.2)\n",
    "alex_response = get_ollama_alex_response(ollama_conversation)\n",
    "ollama_conversation += f\"Alex: {alex_response}\\n\"\n",
    "print(f\"ü§ñ Alex (LLaMA 3.2): {alex_response}\")\n",
    "print()\n",
    "time.sleep(1)\n",
    "\n",
    "# Blake responds (DeepSeek R1)\n",
    "blake_response = get_ollama_blake_response(ollama_conversation)\n",
    "ollama_conversation += f\"Blake: {blake_response}\\n\"\n",
    "print(f\"ü§ñ Blake (DeepSeek R1): {blake_response}\")\n",
    "print()\n",
    "time.sleep(1)\n",
    "\n",
    "# Charlie responds (GPT-OSS)\n",
    "charlie_response = get_ollama_charlie_response(ollama_conversation)\n",
    "ollama_conversation += f\"Charlie: {charlie_response}\\n\"\n",
    "print(f\"ü§ñ Charlie (GPT-OSS): {charlie_response}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887b1aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Ollama conversation\n",
    "ollama_full_conversation = \"\"\n",
    "ollama_topic = \"The Future of Open Source AI\"\n",
    "\n",
    "print(f\"üéØ Topic: {ollama_topic}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üîÑ Complete 3-Way Ollama Conversation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Continue the conversation for several rounds\n",
    "for round_num in range(4):\n",
    "    print(f\"\\n--- Round {round_num + 1} ---\")\n",
    "    \n",
    "    # Alex responds (LLaMA 3.2)\n",
    "    alex_response = get_ollama_alex_response(ollama_full_conversation)\n",
    "    ollama_full_conversation += f\"Alex: {alex_response}\\n\"\n",
    "    print(f\"ü§ñ Alex (LLaMA 3.2): {alex_response}\")\n",
    "    print()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Blake responds (DeepSeek R1)\n",
    "    blake_response = get_ollama_blake_response(ollama_full_conversation)\n",
    "    ollama_full_conversation += f\"Blake: {blake_response}\\n\"\n",
    "    print(f\"ü§ñ Blake (DeepSeek R1): {blake_response}\")\n",
    "    print()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Charlie responds (GPT-OSS)\n",
    "    charlie_response = get_ollama_charlie_response(ollama_full_conversation)\n",
    "    ollama_full_conversation += f\"Charlie: {charlie_response}\\n\"\n",
    "    print(f\"ü§ñ Charlie (GPT-OSS): {charlie_response}\")\n",
    "    print()\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d32d2ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù COMPLETE OLLAMA CONVERSATION HISTORY\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ollama_full_conversation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìù COMPLETE OLLAMA CONVERSATION HISTORY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mollama_full_conversation\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'ollama_full_conversation' is not defined"
     ]
    }
   ],
   "source": [
    "# Display the complete Ollama conversation\n",
    "print(\"\\nüìù COMPLETE OLLAMA CONVERSATION HISTORY\")\n",
    "print(\"=\" * 60)\n",
    "print(ollama_full_conversation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
