{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e79a68a7",
   "metadata": {},
   "source": [
    "# 3-Way Converstation\n",
    "\n",
    "This notebook implements a 3-way conversation between GPT, Claude, and Gemini using the approach suggested in the assignment.\n",
    "\n",
    "### Key Features:\n",
    "- 3 distinct AI personalities with different characteristics\n",
    "- Uses the suggested approach of 1 system prompt + 1 user prompt per model\n",
    "- Includes conversation history in each prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add83d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Get API keys\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "# Initialize clients\n",
    "openai = OpenAI()\n",
    "anthropic = OpenAI(api_key=anthropic_api_key, base_url=\"https://api.anthropic.com/v1/\")\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "\n",
    "print(\"Clients initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d73d7d",
   "metadata": {},
   "source": [
    "Following the suggested approach, we'll use:\n",
    "- 1 system prompt per model\n",
    "- 1 user prompt that includes the full conversation history\n",
    "- Each model responds as their character\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bdefe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the three AI personalities\n",
    "\n",
    "# Alex (GPT) - Argumentative and challenging\n",
    "alex_system_prompt = \"\"\"\n",
    "You are Alex, a chatbot who is very argumentative; you disagree with anything in the conversation and you challenge everything, in a snarky way.\n",
    "You are in a conversation with Blake and Charlie.\n",
    "Keep your responses concise but impactful.\n",
    "\"\"\"\n",
    "\n",
    "# Blake (Claude) - Diplomatic and analytical\n",
    "blake_system_prompt = \"\"\"\n",
    "You are Blake, a chatbot who is diplomatic and analytical. You try to find common ground and provide balanced perspectives.\n",
    "You are in a conversation with Alex and Charlie.\n",
    "You value logic and reason, and try to mediate conflicts.\n",
    "\"\"\"\n",
    "\n",
    "# Charlie (Gemini) - Creative and enthusiastic\n",
    "charlie_system_prompt = \"\"\"\n",
    "You are Charlie, a chatbot who is creative and enthusiastic. You bring energy and new ideas to the conversation.\n",
    "You are in a conversation with Alex and Blake.\n",
    "You love brainstorming and thinking outside the box.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5ca111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get response from Alex (GPT)\n",
    "def get_alex_response(conversation):\n",
    "    user_prompt = f\"\"\"\n",
    "You are Alex, in conversation with Blake and Charlie.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next, as Alex.\n",
    "\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": alex_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", \n",
    "        messages=messages,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d60ceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get response from Blake (Claude)\n",
    "def get_blake_response(conversation):\n",
    "    user_prompt = f\"\"\"\n",
    "You are Blake, in conversation with Alex and Charlie.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next, as Blake.\n",
    "\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": blake_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    response = anthropic.chat.completions.create(\n",
    "        model=\"claude-3-5-haiku-20241022\", \n",
    "        messages=messages,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39fe81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get response from Charlie (Gemini)\n",
    "def get_charlie_response(conversation):\n",
    "    user_prompt = f\"\"\"\n",
    "You are Charlie, in conversation with Alex and Blake.\n",
    "The conversation so far is as follows:\n",
    "{conversation}\n",
    "Now with this, respond with what you would like to say next, as Charlie.\n",
    "\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": charlie_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    \n",
    "    response = gemini.chat.completions.create(\n",
    "        model=\"gemini-2.0-flash-exp\", \n",
    "        messages=messages,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9296a4",
   "metadata": {},
   "source": [
    "## Running the 3-Way Conversation\n",
    "\n",
    "Let's start a conversation about \"The Future of AI in Education\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2019510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize conversation with a topic\n",
    "conversation = \"\"\n",
    "topic = \"The Future of AI in Education\"\n",
    "\n",
    "# Start the conversation\n",
    "print(f\"üéØ Topic: {topic}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Alex starts the conversation\n",
    "alex_response = get_alex_response(conversation)\n",
    "conversation += f\"Alex: {alex_response}\\n\"\n",
    "print(f\"ü§ñ Alex: {alex_response}\")\n",
    "print()\n",
    "\n",
    "# Add a small delay to make it feel more natural\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22946d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blake responds\n",
    "blake_response = get_blake_response(conversation)\n",
    "conversation += f\"Blake: {blake_response}\\n\"\n",
    "print(f\"ü§ñ Blake: {blake_response}\")\n",
    "print()\n",
    "\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26974a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charlie responds\n",
    "charlie_response = get_charlie_response(conversation)\n",
    "conversation += f\"Charlie: {charlie_response}\\n\"\n",
    "print(f\"ü§ñ Charlie: {charlie_response}\")\n",
    "print()\n",
    "\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bfa87e",
   "metadata": {},
   "source": [
    "## Continue the Conversation\n",
    "\n",
    "Let's continue for a few more rounds to see how the personalities interact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127fd194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the conversation for several more rounds\n",
    "for round_num in range(1, 4):\n",
    "    print(f\"--- Round {round_num + 1} ---\")\n",
    "    \n",
    "    # Alex responds\n",
    "    alex_response = get_alex_response(conversation)\n",
    "    conversation += f\"Alex: {alex_response}\\n\"\n",
    "    print(f\"ü§ñ Alex: {alex_response}\")\n",
    "    print()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Blake responds\n",
    "    blake_response = get_blake_response(conversation)\n",
    "    conversation += f\"Blake: {blake_response}\\n\"\n",
    "    print(f\"ü§ñ Blake: {blake_response}\")\n",
    "    print()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Charlie responds\n",
    "    charlie_response = get_charlie_response(conversation)\n",
    "    conversation += f\"Charlie: {charlie_response}\\n\"\n",
    "    print(f\"ü§ñ Charlie: {charlie_response}\")\n",
    "    print()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc1cfe6",
   "metadata": {},
   "source": [
    "## Display Full Conversation History\n",
    "\n",
    "Let's see the complete conversation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìù FULL CONVERSATION HISTORY\")\n",
    "print(\"=\" * 50)\n",
    "print(conversation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
